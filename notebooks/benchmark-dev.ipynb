{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f09ab7b6-7acd-4a16-90b4-68fe3f365458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from retriever import load_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29320f7a-88f4-44fb-91e7-cc7bb0b3f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries, doc_srcs, id_phrases = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8adaea02-740c-4b80-9cf6-5d189b56863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bmq(query, doc_src, id_phrase):\n",
    "    queries.append(query)\n",
    "    doc_srcs.append(doc_src)\n",
    "    id_phrases.append(id_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01db19e3-ded0-4756-8cbb-9b81eb6455e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 70/70 [00:00<00:00, 107.15it/s]\n",
      "INFO:root:Created 2198 chunks from 70 markdown files.\n"
     ]
    }
   ],
   "source": [
    "docs = load_documents('data/', chunk_size=256, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ad824bb-ed57-46dc-9021-4ebd1870434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_documents(doc_src_name, text_desc=None):\n",
    "    for doc in docs:\n",
    "        if Path(doc.metadata['source']).stem == doc_src_name:\n",
    "            if text_desc is None or text_desc in doc.page_content:\n",
    "                print(doc.page_content)\n",
    "                print('='*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e4861-1c27-41e5-bfca-ba8a262c423e",
   "metadata": {},
   "source": [
    "## Textbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d555c48-0971-4a7d-9184-810b97ac8d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are three primary strategies employed for selecting which block to replace:\n",
      "- Random—To spread allocation uniformly, candidate blocks are randomly selected. Some systems generate pseudorandom block numbers to get reproducible behavior, which is particularly useful when debugging hardware.\n",
      "\n",
      "- Least recently used (LRU)—To reduce the chance of throwing out information that will be needed soon, accesses to blocks are recorded. Relying on the past to predict the future, the block replaced is the one that has been unused for the longest time. LRU relies on a corollary of locality: if recently used blocks are likely to be used again, then a good candidate for disposal is the least recently used block.\n",
      "\n",
      "- First in, first out (FIFO)—Because LRU can be complicated to calculate, this approximates LRU by determining the oldest block rather than the LRU.\n",
      "\n",
      "A virtue of random replacement is that it is simple to build in hardware. As the number of blocks to keep track of increases, LRU becomes increasingly expensive and is usually only approximated.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('b.1-b.3', 'There are three primary strategies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a51ea3b-7be2-4989-a9a3-22f5ce4fbcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    'What are the strategies for replacing a cache block?',\n",
    "    'b.1-b.3',\n",
    "    'There are three primary strategies'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c0d6399-2edc-4ab5-9797-be359e4e7ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Four Memory Hierarchy Questions Revisited\n",
      "\n",
      "We are now ready to answer the four memory hierarchy questions for virtual memory.\n",
      "\n",
      "## Q1: Where Can A Block Be Placed In Main Memory?\n",
      "\n",
      "The miss penalty for virtual memory involves access to a rotating magnetic storage device and is therefore quite high. Given the choice of lower miss rates or a simpler placement algorithm, operating systems designers usually pick lower miss rates because of the exorbitant miss penalty. Thus, operating systems allow blocks to be placed anywhere in main memory. According to the terminology in Figure B.2 on page B-8, this strategy would be labeled fully associative.\n",
      "\n",
      "## Q2: How Is A Block Found If It Is In Main Memory?\n",
      "\n",
      "Both paging and segmentation rely on a data structure that is indexed by the page or segment number. This data structure contains the physical address of the block. For segmentation, the offset is added to the segment's physical address to obtain the final physical address. For paging, the offset is simply concatenated to this physical page address (see Figure B.23).\n",
      "\n",
      "This data structure, containing the physical page addresses, usually takes the form of a page table.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('b.4-b.5', 'This data structure, containing the physical page addresses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9124a4d8-4692-49a4-83f4-8cc7d7937678",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    'What does a page table contain?',\n",
    "    'b.4-b.5',\n",
    "    'This data structure, containing the physical page addresses'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c2c678-7707-4ce5-85a0-6b2c5e1b3c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hazards reduce the performance from the ideal speedup gained by pipelining. There are three classes of hazards:\n",
      "1. Structural hazards arise from resource conflicts when the hardware cannot support all possible combinations of instructions simultaneously in overlapped execution. In modern processors, structural hazards occur primarily in special purpose functional units that are less frequently used (such as floating point divide or other complex long running instructions). They are not a major performance factor, assuming programmers and compiler writers are aware of the lower throughput of these instructions. Instead of spending more time on this infrequent case, we focus on the two other hazards that are much more frequent.\n",
      "\n",
      "2. Data hazards arise when an instruction depends on the results of a previous instruction in a way that is exposed by the overlapping of instructions in the pipeline.\n",
      "\n",
      "3. Control hazards arise from the pipelining of branches and other instructions that change the PC.\n",
      "\n",
      "Hazards in pipelines can make it necessary to stall the pipeline. Avoiding a hazard often requires that some instructions in the pipeline be allowed to proceed while others are delayed.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('c.1-c.4', 'There are three classes of hazards:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55283e24-d145-4f67-9d04-8d868d944334",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    'What are the 3 pipeline hazards?',\n",
    "    'c.1-c.4',\n",
    "    'There are three classes of hazards:'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "729d1d49-1b3a-4e0a-82ce-13a1761d8cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single instruction stream, single data stream (SISD)—This category is the uniprocessor. The programmer thinks of it as the standard sequential computer, but it can exploit ILP. Chapter 3 covers SISD architectures that use ILP techniques such as superscalar and speculative execution.\n",
      "\n",
      "2. Single instruction stream, multiple data streams (SIMD)—The same instruction is executed by multiple processors using different data streams. SIMD computers exploit data-level parallelism by applying the same operations to multiple items of data in parallel. Each processor has its own data memory\n",
      "(hence, the MD of SIMD), but there is a single instruction memory and control processor, which fetches and dispatches instructions. Chapter 4 covers DLP and three different architectures that exploit it: vector architectures, multimedia extensions to standard instruction sets, and GPUs.\n",
      "\n",
      "3. Multiple instruction streams, single data stream (MISD)—No commercial multiprocessor of this type has been built to date, but it rounds out this simple classification.\n",
      "\n",
      "4. Multiple instruction streams, multiple data streams (MIMD)—Each processor fetches its own instructions and operates on its own data, and it targets task-level parallelism.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('ch1.1-1.6', 'exploit data-level parallelism by applying the same operations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f1c361f-e3c5-4bde-86ea-9ca6b5f76068",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    'What parallelism does SIMD exploit?',\n",
    "    'ch1.1-1.6',\n",
    "    'exploit data-level parallelism by applying the same operations'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "002d8891-17f2-4465-ae06-31bfa835cae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example, they would pay the customer a penalty if they did not meet an agreement of some hours per month. Thus an SLA could be used to decide whether the system was up or down.\n",
      "\n",
      "Systems alternate between two states of service with respect to an SLA:\n",
      "1. Service accomplishment, where the service is delivered as specified. 2. Service interruption, where the delivered service is different from the SLA.\n",
      "\n",
      "Transitions between these two states are caused by failures (from state 1 to state 2)\n",
      "or restorations (2 to 1). Quantifying these transitions leads to the two main measures of dependability:\n",
      "- Module reliability is a measure of the continuous service accomplishment (or, equivalently, of the time to failure) from a reference initial instant. Therefore the mean time to failure (MTTF) is a reliability measure. The reciprocal of MTTF is a rate of failures, generally reported as failures per billion hours of operation, or FIT (for failures in time). Thus an MTTF of 1,000,000 hours equals 109/106 or 1000 FIT. Service interruption is measured as mean time to repair (MTTR).\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('ch1.7', 'For example, they would pay the customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c511ce0-ce7a-4c5e-85c1-827a848e2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    'Can you give me an example of service level agreement?',\n",
    "    'ch1.7',\n",
    "    'For example, they would pay the customer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24de7a89-2c26-41fb-bb0c-984808de7b88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amdahl's Law states that the performance improvement to be gained from using some faster mode of execution is limited by the fraction of the time the faster mode can be used.\n",
      "\n",
      "Amdahl's Law defines the speedup that can be gained by using a particular feature. What is speedup? Suppose that we can make an enhancement to a computer that will improve performance when it is used. Speedup is the ratio Speedup ¼ Performance for entire task using the enhancement when possible Performance for entire task without using the enhancement Alternatively, Speedup ¼ Execution time for entire task without using the enhancement Execution time for entire task using the enhancement when possible Speedup tells us how much faster a task will run using the computer with the enhancement contrary to the original computer.\n",
      "\n",
      "Amdahl's Law gives us a quick way to find the speedup from some enhancement, which depends on two factors:\n",
      "\n",
      "1. The fraction of the computation time in the original computer that can be converted to take advantage of the enhancement—For example, if 40 seconds of the execution time of a program that takes 100 seconds in total can use an enhancement, the fraction is 40/100.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('ch1.8-1.9', \"Amdahl's Law states that\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3ffda09-cf07-4f64-8b57-7d654a302845",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"What is the definition of Amdahl's Law?\",\n",
    "    'ch1.8-1.9',\n",
    "    \"Amdahl's Law states that\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef39c608-d9af-4b4a-8f03-5fa30fa00d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the miss penalty is small, the compiler just unrolls the loop once or twice, and it schedules the prefetches with the execution. If the miss penalty is large, it uses software pipelining (see Appendix H) or unrolls many times to prefetch data for a future iteration.\n",
      "\n",
      "Issuing prefetch instructions incurs an instruction overhead, however, so compilers must take care to ensure that such overheads do not exceed the benefits.\n",
      "\n",
      "By concentrating on references that are likely to be cache misses, programs can avoid unnecessary prefetches while improving average memory access time significantly.\n",
      "\n",
      "2.3 Ten Advanced Optimizations of Cache Performance - 111 Example For the following code, determine which accesses are likely to cause data cache misses. Next, insert prefetch instructions to reduce misses. Finally, calculate the number of prefetch instructions executed and the misses avoided by prefetching.\n",
      "\n",
      "Let's assume we have an 8 KiB direct-mapped data cache with 16-byte blocks, and it is a write-back cache that does write allocate. The elements of a and b are 8 bytes long because they are double-precision floating-point arrays.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('ch2.3', 'If the miss penalty is small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4119431-7f17-49d8-b131-927bea003c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"What prefetch optimizations does the compiler perform on loops when the miss penalty is small?\",\n",
    "    \"ch2.3\",\n",
    "    'If the miss penalty is small'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33f23923-cfdc-4c31-95f0-eebfe495fc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most often, the VM supports the same ISA as the underlying hardware; however, it is also possible to support a different ISA, and such approaches are often employed when migrating between ISAs in order to allow software from the departing ISA to be used until it can be ported to the new ISA. Our focus here will be on VMs where the ISA presented by the VM and the underlying hardware match. Such VMs are called (operating) system virtual machines. IBM VM/370, VMware ESX Server, and Xen are examples. They present the illusion that the users of a VM have an entire computer to themselves, including a copy of the operating system. A single computer runs multiple VMs and can support a number of different operating systems (OSes). On a conventional platform, a single OS \"owns\" all the hardware resources, but with a VM, multiple OSes all share the hardware resources.\n",
      "\n",
      "The software that supports VMs is called a virtual machine monitor (VMM) or hypervisor; the VMM is the heart of virtual machine technology. The underlying hardware platform is called the host, and its resources are shared among the guest VMs.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('ch2.4', 'Such VMs are called')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ce1cd38-6b70-4257-88e5-d4c6cc7e967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"What are some examples of system virtual machines?\",\n",
    "    \"ch2.4\",\n",
    "    'Such VMs are called'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "218d338b-94b6-4ba9-a57f-d23f4eaa1b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perhaps the best-known example of a correlating predictor is McFarling's gshare predictor. In gshare the index is formed by combining the address of the branch and the most recent conditional branch outcomes using an exclusiveOR, which essentially acts as a hash of the branch address and the branch history.\n",
      "\n",
      "The hashed result is used to index a prediction array of 2-bit counters, as shown in Figure 3.4. The gshare predictor works remarkably well for a simple predictor, and is often used as the baseline for comparison with more sophisticated predictors.\n",
      "\n",
      "Predictors that combine local branch information and global branch history are also called alloyed predictors or hybrid predictors.\n",
      "\n",
      "## Tournament Predictors: Adaptively Combining Local And Global Predictors\n",
      "\n",
      "The primary motivation for correlating branch predictors came from the observation that the standard 2-bit predictor, using only local information, failed on some important branches. Adding global history could help remedy this situation.\n",
      "\n",
      "Tournament predictors take this insight to the next level, by using multiple predictors, usually a global predictor and a local predictor, and choosing between them\n",
      "\n",
      "![3_image_0.png](3_image_0.png)\n",
      "\n",
      "with a selector, as shown in Figure 3.5.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('ch3.3', 'Tournament predictors take this insight to the next level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54a2fdca-8e08-40ed-8cff-aa415451aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"What is a tournament predictor?\",\n",
    "    \"ch3.3\",\n",
    "    'Tournament predictors take this insight to the next level'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "baf24b40-340c-4569-97eb-3de605413ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception is imprecise if the processor state when an exception is raised does not look exactly as if the instructions were executed sequentially in strict program order. Imprecise exceptions can occur because of two possibilities:\n",
      "1. The pipeline may have already completed instructions that are later in program order than the instruction causing the exception.\n",
      "\n",
      "2. The pipeline may have not yet completed some instructions that are earlier in program order than the instruction causing the exception.\n",
      "\n",
      "Imprecise exceptions make it difficult to restart execution after an exception.\n",
      "\n",
      "Rather than address these problems in this section, we will discuss a solution that provides precise exceptions in the context of a processor with speculation in Section 3.6. For floating-point exceptions, other solutions have been used, as discussed in Appendix J.\n",
      "\n",
      "To allow out-of-order execution, we essentially split the ID pipe stage of our simple five-stage pipeline into two stages:\n",
      "\n",
      "## 1. Issue—Decode Instructions, Check For Structural Hazards.\n",
      "\n",
      "2. Read operands—Wait until no data hazards, then read operands.\n",
      "\n",
      "An instruction fetch stage precedes the issue stage and may fetch either to an instruction register or into a queue of pending instructions; instructions are then issued from the register or queue.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('ch3.4', 'Imprecise exceptions can occur because of two possibilities:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e49dbba7-6819-47ae-8f08-28dff4aa6693",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"How can imprecise exceptions by dynamically scheduled processors occur?\",\n",
    "    \"ch3.4\",\n",
    "    'Imprecise exceptions can occur because of two possibilities:'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3978cc09-adc6-4c12-9fa9-86ddc9682378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy consumption is determined by the combination of speedup and increase in power consumption. For the Java benchmarks, on average, SMT delivers the same energy efficiency as non-SMT (average of 1.0), but it is brought down by the two poor performing benchmarks; without pjbb2005 and tradebeans, the average energy efficiency for the Java benchmarks is 1.06, which is almost as good as the PARSEC benchmarks. In the PARSEC benchmarks, SMT reduces energy by 1!(1/1.08)¼7%. Such energy-reducing performance enhancements are very difficult to find. Of course, the static power associated with SMT is paid in both cases, thus the results probably slightly overstate the energy gains.\n",
      "\n",
      "These results clearly show that SMT with extensive support in an aggressive speculative processor can improve performance in an energy-efficient fashion. In 2011, the balance between offering multiple simpler cores and fewer more sophisticated cores has shifted in favor of more cores, with each core typically being a three- to four-issue superscalar with SMT supporting two to four threads. Indeed, Esmaeilzadeh et al.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('ch3.11', 'Energy consumption is determined by the combination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d48979a5-70f6-4cec-b8ab-4368ff7b4be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"Can SMT improve performance in an energy-efficient way?\",\n",
    "    \"ch3.11\",\n",
    "    'Energy consumption is determined by the combination'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7847da0-1cde-44eb-b999-aa96ba775bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second effect, called false sharing, arises from the use of an invalidationbased coherence algorithm with a single valid bit per cache block. False sharing occurs when a block is invalidated (and a subsequent reference causes a miss)\n",
      "because some word in the block, other than the one being read, is written into.\n",
      "\n",
      "If the word written into is actually used by the processor that received the invalidate, then the reference was a true sharing reference and would have caused a miss independent of the block size. If, however, the word being written and the word read are different and the invalidation does not cause a new value to be communicated, but only causes an extra cache miss, then it is a false sharing miss. In a false sharing miss, the block is shared, but no word in the cache is actually shared, and the miss would not occur if the block size were a single word. The following example makes the sharing patterns clear.\n",
      "\n",
      "Example Assume that words z1 and z2 are in the same cache block, which is in the shared state in the caches of both P1 and P2. Assuming the following sequence of events, identify each miss as a true sharing miss, a false sharing miss, or a hit.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('ch5.1-5.3', 'False sharing occurs when a block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6c64689-3da0-461c-957a-0bce3ec250df",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"When does false sharing occur?\",\n",
    "    \"ch5.1-5.3\",\n",
    "    'False sharing occurs when a block'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "908dd54b-f6dd-40b4-b828-a033d4a085ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In particular, only register-register instructions can safely be permitted; otherwise, it is possible to create deadlock situations where the processor can never complete the sc. In addition, the number of instructions between the load reserved and the store conditional should be small to minimize the probability that either an unrelated event or a competing processor causes the store conditional to fail frequently.\n",
      "\n",
      "## Implementing Locks Using Coherence\n",
      "\n",
      "Once we have an atomic operation, we can use the coherence mechanisms of a multiprocessor to implement spin locks—locks that a processor continuously tries to acquire, spinning around a loop until it succeeds. Spin locks are used when programmers expect the lock to be held for a very short amount of time and when they want the process of locking to be low latency when the lock is available. Because spin locks tie up the processor waiting in a loop for the lock to become free, they are inappropriate in some circumstances.\n",
      "\n",
      "The simplest implementation, which we would use if there were no cache coherence, would be to keep the lock variables in memory. A processor could continually try to acquire the lock using an atomic operation, say, atomic exchange, and test whether the exchange returned the lock as free.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('ch5.5-5.6', 'locks that a processor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28ec8044-004b-4fc9-a351-20c325133167",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"What is a spin lock?\",\n",
    "    \"ch5.5-5.6\",\n",
    "    'locks that a processor'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20943b07-8612-4827-a38f-85a342329bb7",
   "metadata": {},
   "source": [
    "## Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d3ae6e8-10a0-43cc-b6f8-c5fd80574e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The source can be an immediate value, a register, or a memory location (expressed using one of the addressing mode expressions from above). The destination is either a register or a memory location. At most one of source or destination can be memory. The `mov` suffix (b, w, l, or q) indicates how many bytes are being copied (1, 2, 4, or 8 respectively). For the `lea` (load effective address) instruction, the source operand is a memory location (using an addressing mode from above) and it copies the calculated source _address_ to destination. Note that `lea` does not dereference the source address, it simply calculates its location. This means `lea` is nothing more than an arithmetic operation and commonly used to calculate the value of simple linear combinations that have nothing to do with memory locations!\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('guide-to-x86-64', 'the source operand is a memory location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87571346-39d6-4967-bc29-7b7b69fc2dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"Does the lea instruction access data memory?\",\n",
    "    \"guide-to-x86-64\",\n",
    "    'the source operand is a memory location'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e663789-67e8-47a1-bead-4975d6f64521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When a miss occurs, data is loaded into both the miss cache and the direct-mapped cache. In a sense, this duplication of data wastes storage space in the miss cache. The number of duplicate items in the miss cache can range from one (in the case where all items in the miss cache map to the same line in the direct-mapped cache) to all of the entries (in the case where a series of misses occur which do not hit in the miss cache).\n",
      "\n",
      "To make better use of the miss cache we can use a different replacement algorithm for the small fullyassociative cache [5]. Instead of loading the requested data into the miss cache on a miss, we can load the fully-associative cache with the victim line from the direct-mapped cache instead. We call this victim caching\n",
      "(see Figure 3-4). With victim caching, no data line appears both in the direct-mapped cache and the victim cache. This follows from the fact that the victim cache is loaded only with items thrown out from the directmapped cache. In the case of a miss in the directmapped cache that hits in the victim cache, the contents of the direct-mapped cache line and the matching victim cache line are swapped.\n",
      "\n",
      "!\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents(\n",
    "    'Improving_direct-mapped_cache_performance_by_the_addition_of_a_small_fully-associative_cache_and_prefetch_buffers',\n",
    "    'Instead of loading the requested data'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3bfa30d3-509a-426c-96fe-46f91f0ca8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"What does victim caching load when there is a cache miss?\",\n",
    "    \"Improving_direct-mapped_cache_performance_by_the_addition_of_a_small_fully-associative_cache_and_prefetch_buffers\",\n",
    "    'Instead of loading the requested data'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "017df099-0a2c-42a2-be73-86fa9be6094b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An understanding of the predictability of branches may lead to insights ultimately resulting in better or less complex predictors. We also investigate and quantify what fraction of the branches in each benchmark is predictable using each of the methods described in this paper.\n",
      "\n",
      "## 1. Introduction\n",
      "\n",
      "To build high performance microprocessors, accurate branch prediction is required. The correct prediction of branch outcomes and targets is necessary to avoid pipeline bubbles. Over the years, several branch prediction strategies have been proposed [8, 11, 2] to improve prediction accuracy. Several researchers have proposed modifications [2, 3, 7] to these schemes, and there have been studies on which configurations of these work best [6]. However, there has been little work that explains what makes branches predictable. A better understanding of this would likely lead to insights ultimately resulting in better predictors, or in reducing the complexity of current predictors.\n",
      "\n",
      "Global two-level branch predictors such as GAs [11]\n",
      "and gshare [2] take advantage of the correlation between branches. Pan, So, and Rahmeh [4] identified several cases of such correlated branches in the SPEC89 benchmarks.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents(\n",
    "    \"An_analysis_of_correlation_and_predictability_what_makes_two-level_branch_predictors_work\",\n",
    "    'take advantage of the correlation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e0b7fec-312f-40a5-946d-d74b54de7b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"What do two-level predictors take advantage of?\",\n",
    "    \"An_analysis_of_correlation_and_predictability_what_makes_two-level_branch_predictors_work\",\n",
    "    'take advantage of the correlation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d22f498-acc8-4d9e-9405-060058fa302b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hence, the central processing unit\n",
      "(CPU) includes five LEA units as well as five integer ALUs. On the vector side, new fast adders were added on ports 1 and 5. These are three-cycle fast adders, with two cycles bypass between back-to-back floatingpoint ADD operations. Port 11 provides a third load port with a dedicated address-generation unit. This allows the L1 data cache to supply more data to the new wideexecution machine.\n",
      "\n",
      "## P-Core Memory Subsystem\n",
      "\n",
      "The P-core memory subsystem is designed to handle up to three 32-byte loads or two up to 64-byte loads per cycle, as well as up to two 64-byte stores per cycle, providing simultaneous 128 bytes of read bandwidth and 128 bytes of write bandwidth per cycle. The L1 load-to-use latency is five cycles.\n",
      "\n",
      "Deeper load buffer and store buffer expose more parallelism for load and store operations.\n",
      "\n",
      "Prediction of loads that are independent of previous stores is improved, and the penalty for wrong prediction is significantly reduced. Store-to-load forwarding is supported, as well as allowing cases where the first load bytes are forwarded from a store, and the rest are read from the data cache.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents(\n",
    "    \"Intel_Alder_Lake_CPU_Architectures\",\n",
    "    'The P-core memory subsystem is designed to handle'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5e14d11-ea12-427f-af53-8be088c06802",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"How many 32-byte loads is the Intel Alder Lake architecture P-core memory subsystem design to handle?\",\n",
    "    \"Intel_Alder_Lake_CPU_Architectures\",\n",
    "    'The P-core memory subsystem is designed to handle'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b564072b-933c-4e49-9b64-d036861f057f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tradeoff arises because supporting more hardware contexts leaves fewer (excess) registers available for renaming. The number of renaming registers, however, determines the total number of instructions the processor can have in-flight. It is difficult to predict the right register file size that far into the future, but in Figure 7 we illustrate this type of analysis by linding the performance achieved with 200 physical registers. \n",
      "\n",
      "That equates to a I -thread machine with 1 68 excess registers or a 4-thread machine with 72 excess registers, for example. In this case there is a clear maximum point at 4 threads. \n",
      "\n",
      "In summary, fetch throughput is still a bottleneck in our proposed architecture. It may no longer be appropriate to keep fetch and issue bandwidth in balance, given the much greater difficulty of filling the fetch bandwidth. Also, register file access time will likely be a limiting factor in the number of threads an architecture can support. \n",
      "\n",
      "## 8 Related Work\n",
      "\n",
      "A number of other architecturcs have been proposed that exhibit simultaneous multithreading in some form. Thllsen, et al., [27] \n",
      "demonstrated the potential for simultaneous multithreading.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents(\n",
    "    \"Exploiting_Choice_Instruction_Fetch_and_Issue_on_an_Implementable_Simultaneous_Multithreading_Processor\",\n",
    "    'In summary, fetch'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "361f2689-1dd8-4043-8ee8-ff5dce197e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"Is fetch throughput a main bottleneck in SMT architectures?\",\n",
    "    \"Exploiting_Choice_Instruction_Fetch_and_Issue_on_an_Implementable_Simultaneous_Multithreading_Processor\",\n",
    "    'In summary, fetch'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2d81a40-4882-4107-8139-2db25f6c42da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While placing FPGAs as a network-side \"bump-in-the-wire\" solves many of the shortcomings of the torus topology, much more is possible. By enabling the FPGAs to generate and consume their own networking packets independent of the hosts, each and every FPGA in the datacenter can reach every other one (at a scale of hundreds of thousands) in a small number of microseconds, without any intervening software. This capability allows hosts to use remote FPGAs for acceleration with low latency, improving the economics of the accelerator deployment, as hosts running services that do not use their local FPGAs can donate them to a global pool and extract value which would otherwise be stranded. Moreover, this design choice essentially turns the distributed FPGA\n",
      "resources into an independent computer in the datacenter, at the same scale as the servers, that physically shares the network wires with software. Figure 1a shows a logical view of this plane of computation.\n",
      "\n",
      "This model offers significant flexibility. From the local perspective, the FPGA is used as a compute or a network accelerator. From the global perspective, the FPGAs can be managed as a large-scale pool of resources, with acceleration services mapped to remote FPGA resources.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents(\n",
    "    \"A_cloud-scale_acceleration_architecture\",\n",
    "    'each and every FPGA'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03880772-7075-4dea-9929-c07236d59900",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"Does placing FPGAs as a network-side bump-in-the-wire improve latency?\",\n",
    "    \"A_cloud-scale_acceleration_architecture\",\n",
    "    'each and every FPGA'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7adb3027-4218-44a4-8d43-25b6c48ef9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, we set power to be the\n",
      "\"only\" constraint, and vary the level of parallelism in the PARSEC applications from 0.75 to 0.99, assuming programmer effort can somehow realize this. As shown in Figure 6(a), which normalizes speedup to a quad-core Nehalem at 45 nm, we see performance improves only slowly as the parallelism level increases, with most benchmarks reaching a speedup of about only 15× at 99% parallelism. The markers show the level of parallelism in their current implementation. If power was the only constraint, typical ITRSscaling speedups would still be limited to 15×. With conservative scaling, this best-case speedup is 6.3×.\n",
      "\n",
      "We then see what happens if parallelism alone was the constraint by allowing the power budget to vary from 50 W to 500 W (our default budget is 125 W) in Figure 6(b). Eight of twelve benchmarks show no more than 10X speedup even with practically unlimited power, i.e. parallelism is the primary contributor to dark silicon.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents(\n",
    "    \"Dark Silicon and the End of Multicore Scaling\",\n",
    "    'Eight of twelve'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2040d50-4ab4-45c7-a37e-da656eca41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"What is the primary source of dark silicon?\",\n",
    "    \"Dark Silicon and the End of Multicore Scaling\",\n",
    "    'Eight of twelve'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d3877ae-ba41-4dfe-b7a3-2d3ec48659da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy savings are substantially more than chip-wide voltage/frequency scaling.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "As processors continue to increase in performance and speed, processor power consumption and heat dissipation have become key challenges in the design of future highperformance systems. For example, Pentium-4 class processors currently consume well over 50W and processors in the year 2015 are expected to consume close to 300W [1].\n",
      "\n",
      "Increased power consumption and heat dissipation typically leads to higher costs for thermal packaging, fans, electricity, and even air conditioning. Higher-power systems can also have a greater incidence of failures.\n",
      "\n",
      "In this paper, we propose and evaluate a *single-ISA heterogeneous multi-core architecture* [26, 27] to reduce processor power dissipation. Prior chip-level multiprocessors (CMP) have been proposed using multiple copies of the same core (i.e., homogeneous), or processors with coprocessors that execute a different instruction set. We propose that for many applications, core diversity is of higher value than uniformity, offering much greater ability to adapt to the demands of the application(s). We present a multicore architecture where all cores execute the same instruction set, but have different capabilities and performance levels.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents(\n",
    "    \"Single-ISA_heterogeneous_multi-core_architectures_the_potential_for_processor_power_reduction\",\n",
    "    'offering much greater ability to adapt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9327fbf4-c53e-4cc9-a4ba-ff4400bcf2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"What are the advantages of core diversity compared to chip-level multiprocessors with homogeneous cores?\",\n",
    "    \"Single-ISA_heterogeneous_multi-core_architectures_the_potential_for_processor_power_reduction\",\n",
    "    'offering much greater ability to adapt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f35897-7a2a-4039-996e-d29ff9a1a00f",
   "metadata": {},
   "source": [
    "## Assignment Clarification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2528bbef-c140-4ef8-9d3c-5058e4bd30aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the total dynamic instruction count, dynamic memory instruction count, and dynamic branch / jump instruction count for function `loop_fn`?\n",
      "One can calculate the total instruction count by multiplying instruction count of a basic block and the number of times it executed.\n",
      "Dynamic instruction count is the number of instructions that the processor executes and \"static instructions\" is the number of instructions the compiler generates.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('asmt1-q10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47eaa5a2-a8e7-400a-9a9b-3365d1b82006",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"What is dynamic instruction count for loop_fn?\",\n",
    "    \"asmt1-q10\",\n",
    "    ''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a9c8e0f-a0b8-43a7-9388-5a95afedaf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall that Amdahl's Law limits the speedup an optimization can provide. Precisely, S_tot = 1 / (x / S + (1 - x)), where S is the speedup the optimization provides, x is the fraction of total execution time affected by the optimization, and S_tot is the total speedup.\n",
      "Here we have a function `everything` that calls into two functions: `matrix_column_major` and `baseline_int`.\n",
      "Based on the data you collected earlier in Question 8 and 10 for the performances of `baseline_int()`, `baseline_int_cuda()`, `matrix_column_major()`, and `matrix_row_major()`, use Amdahl's law to predict the speedup of:\n",
      "- Option 1: Replacing `baseline_int()` with `baseline_int_cuda()`\n",
      "- Option 2: Replacing `matrix_column_major()` with `matrix_row_major()`\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('asmt2-q11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a41d5908-2a41-4de1-9de4-24ef7e6cd217",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"What do I replace to calculate the speedup of the function everything?\",\n",
    "    \"asmt2-q11\",\n",
    "    ''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da6ed089-abd0-4ef4-9b2c-2cc666c0850b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare the ratio of FLOPS and latency of running `baseline_double` and `baseline_double_cuda` functions, do they match? Why and why not?\n",
      "Step 1: Calculate the ratio between FLOPS of `baseline_double` and FLOPS of `baseline_double_cuda`\n",
      "Step 2: Calculate the ratio between latency of `baseline_double` and latency of `baseline_double_cuda`\n",
      "Step 3: Compare the 2 ratios, do they match? Why or why not?\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('asmt2-q14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "139d1090-ab15-4ff8-882b-153c8a015de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"What is the ratio of FLOPS and latency of running `baseline_double` and `baseline_double_cuda` functions?\",\n",
    "    \"asmt2-q14\",\n",
    "    ''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67203021-6b8a-487d-8337-dd9571bcf0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Meeting Our Processor Again\n",
      "\n",
      "In this assignment, we again use the Intel Core i3-12100F \"Alder Lake\" processor that you've been used before. Now, as you have more knowledge about computer architectures, let's take a look of the processor detail again and see if you have different feelings.\n",
      "! cs203 run 'lscpu'\n",
      "As we learned in the lectures, the cache configuration determines the performance of executing code. Knowing the cache configurations and the parameters -- A, B, C, S would help us to better predict how memory accesses will behave when executing code. The following command will help us getting these important cache configurations and parameters on the bare metal cloud machine.\n",
      "!cs203 run ' getconf -a | grep CACHE'\n",
      "\n",
      "# Question\n",
      "Reading the output from the above command, let's focus on \"LEVEL1_D\", namely, the L1 data cache configuration first.\n",
      "Remember the C = ABS equation we taught in class, what are the values of C, A, B, S, the number of offset bits, and the number of index bits?\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('asmt3-q1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73d821e0-df75-46df-9c5f-0d28f1696aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"How do I get the value of LEVEL1_D?\",\n",
    "    \"asmt3-q1\",\n",
    "    ''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d7008f4-235f-4d37-ac42-6c8a4801de15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the function `uint64_t* array(uint64_t * data, uint64_t size, uint64_t arg1)` and assume the register assignment is turned on.\n",
      "Using the cache configuration we've learned in Question 1, if `size == 8192`, how many total L1 data cache misses do think will occur during the execution of `array`?\n",
      "Assume the cache is empty to begin and you may assume the `data` address starts with something like 0x40000 as we did in the lecture.\n",
      "Start with filling the table with the first 16 memory accesses.\n",
      "What's the estimate cache miss rate? Please explain.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('asmt3-q3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2bc38d52-5418-40d0-ba44-50a4206d1024",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"What cache configuration do I need to use to calculate the L1 data cache misses?\",\n",
    "    \"asmt3-q3\",\n",
    "    ''\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "02a55aa8-f665-479a-98ac-5fdf67c37c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Miss Machine\n",
      "Let's use a miss machine to measure the L1 TLB miss latency.\n",
      "\n",
      "1. It has a template-configurable link size (`BYTES`).\n",
      "2. We allocate the `MM` links in array that 4096-byte aligned.\n",
      "3. We use the function `madvise` to prevent us from using 2MB pages, which Linux will automatically use when it can.\n",
      "4. We can set the total size of the miss machine in bytes with the `size` parameter. It should be a multiple of `BYTES`.\n",
      "\n",
      "# Question\n",
      "Using the code above, what values of `BYTES` and `size` should we run the miss machine with to measure the L1-TLB miss latency?\n",
      "(The fact that there are two experiments listed is a hint that you'll need to run two different experiments.)\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('asmt3-q16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86d98bdc-2e66-4560-90d7-e207b691f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"What does BYTES represent?\",\n",
    "    \"asmt3-q16\",\n",
    "    ''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c62e2b84-85aa-4f80-8c0c-773156aed769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the above function `array` with an argument `data_size`, `data_size=131072`, how many branch instructions are there in `array`?\n",
      "Assume the processor uses an advanced branch predictor. What's the accuracy of branch prediction?\n",
      "This advanced branch predictor:\n",
      "- Is similar to the tournament predictor we in class\n",
      "- Has a resonably long history and high accuracy, so it learns the pattern in a neglible number of steps if the pattern is predictable\n",
      "Hint: Does the content of `data` affect the prediction accuracy?\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('asmt4-q3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "674b2611-33ce-4e7f-80d2-b767777f1622",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"What can I assume about the advanced branch predictor?\",\n",
    "    \"asmt4-q3\",\n",
    "    ''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4058334f-d851-4d71-8884-be141a6e3218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this assignment, we are optimizing a query that returns the discount based on the order quantity.\n",
      "\n",
      "Requirments:\n",
      "1. Values in the source data arrays are 32-bit integers\n",
      "2. The range of order quantity is 0 to 127\n",
      "3. The discount values are stored as data type double\n",
      "4. The performance target is 2.5x on datahub and 2x on Gradescope. Only the score you got on Gradescope counts\n",
      "\n",
      "Things To Try\n",
      "1. Exploit instruction parallelism of the code by breaking the data dependency chains to allow parallel memory accesses and parallel ALU executions\n",
      "2. Reduce branch misses by reducing the branches, if not eliminating all of them\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('asmt4-prog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5665e1f9-a983-4dec-b34b-d240800a95e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"How much speedup on Gradescope do I need to achieve for the discount query program?\",\n",
    "    \"asmt4-prog\",\n",
    "    ''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9eec3c-53e3-4530-9647-674ca52cbfd3",
   "metadata": {},
   "source": [
    "## Logistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a6e598f0-a68e-44b6-9d14-c6ac3752dd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# CS 203: Advanced Computer Architecture 2024 Spring\n",
      "\n",
      "## Instructors\n",
      "- The class instructor is Professor Hung-Wei Tseng. His office hour is every Wednesday 2pm to 4pm at Winston Chung Hall Room 406.\n",
      "- The teaching assistant is Nurlan Nazaraliyev. The TA's office hour is every Monday 2pm to 4pm at Winston Chung Hall Room 110.\n",
      "You can contact the instructor or the TA through this email: cs203 @ escalab.org\n",
      "\n",
      "## Course Overview\n",
      "This course will describe the basics of modern processor operation and techniques to optimize your applications.\n",
      "Topics include computer system performance, instruction set architectures, pipelining, branch prediction, memory-hierarchy design, and a brief introduction to multiprocessor architecture issues.\n",
      "\n",
      "## Materials\n",
      "The textbook the course uses is Patterson & Hennessy, Computer Architecture: A Quantitative Approach, David Patterson & John Hennessy, Morgan Kaufmann, 6th Edition. In addition to the textbook, other research papers assigned throughout the quarter.\n",
      "\n",
      "## Grading\n",
      "\n",
      "### Assignments\n",
      "- Assignments consist 25% of your grade.\n",
      "- Assignments will be assigned throughout the course. We will drop your lowest one.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('syllabus', 'office hour is every Wednesday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e97cb92-7c5a-4ce3-b335-5f656d998129",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"When is the professor's office hour?\",\n",
    "    \"syllabus\",\n",
    "    'office hour is every Wednesday'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "165a0c28-3f07-4e2d-a0d0-42a82d9b5e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# CS 203: Advanced Computer Architecture 2024 Spring\n",
      "\n",
      "## Instructors\n",
      "- The class instructor is Professor Hung-Wei Tseng. His office hour is every Wednesday 2pm to 4pm at Winston Chung Hall Room 406.\n",
      "- The teaching assistant is Nurlan Nazaraliyev. The TA's office hour is every Monday 2pm to 4pm at Winston Chung Hall Room 110.\n",
      "You can contact the instructor or the TA through this email: cs203 @ escalab.org\n",
      "\n",
      "## Course Overview\n",
      "This course will describe the basics of modern processor operation and techniques to optimize your applications.\n",
      "Topics include computer system performance, instruction set architectures, pipelining, branch prediction, memory-hierarchy design, and a brief introduction to multiprocessor architecture issues.\n",
      "\n",
      "## Materials\n",
      "The textbook the course uses is Patterson & Hennessy, Computer Architecture: A Quantitative Approach, David Patterson & John Hennessy, Morgan Kaufmann, 6th Edition. In addition to the textbook, other research papers assigned throughout the quarter.\n",
      "\n",
      "## Grading\n",
      "\n",
      "### Assignments\n",
      "- Assignments consist 25% of your grade.\n",
      "- Assignments will be assigned throughout the course. We will drop your lowest one.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('syllabus', 'Assignments consist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c07b3143-2835-4e1e-8967-a879cec12dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"What is the percentage of assignments?\",\n",
    "    \"syllabus\",\n",
    "    'Assignments consist'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5eada3d2-bb52-46ad-944f-3dce417d278c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Our assignments will be based on Jupyterhub/Jupyter notebooks and programming — everyone is supposed to have a unique turnin. There is no group submission or collaboration allowed.\n",
      "- There is no regrading on assignments except for obvious grading errors. Please make sure your answers are clearly comprehensible to our grading staffs and follow the guideline of answering assignment questions.\n",
      "\n",
      "### Peer Instruction\n",
      "- This class uses “peer instruction” as our teaching method.\n",
      "- To encourage your participation in peer instructions, we count your participation in peer instructions throughout the quarter as one assignment. In other words, if you opt out from peer instructions, this one will become the one that is dropped from your assignments.\n",
      "- To participate in peer instructions, we require each of you to download poll everywhere App or navigate/login to their website during the class if you would like to earn credits from participating in peer instructions.\n",
      "- You must login with UCRNetID@ucr.edu. If you didn’t do it right, you won’t get credits.\n",
      "- You need to answer 50% of the poll questions to receive full credits on peer instructions.\n",
      "\n",
      "### Reading Quizzes\n",
      "- Reading quizzes consists 15% of your grade. We will have reading quizzes on GradeScope.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('syllabus', 'Peer Instruction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ec0162e3-cbdd-4d79-bf3d-885a0265d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"How does peer instruction work?\",\n",
    "    \"syllabus\",\n",
    "    'Peer Instruction'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3a26fab5-c4fb-43b4-b980-171dccb5019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Final Exam Logistics\n",
      "\n",
      "## Time and Location\n",
      "- The location of the final exam is to be announced\n",
      "- The final exam will be on June 14th 8am - 11am\n",
      "\n",
      "## During the Exam\n",
      "- You’re strongly encouraged to arrive at least 10 minutes earlier as you will have to go through the following before you can start working on your exam:\n",
      "  - Pick up the exam from the TA/instructor\n",
      "  - Sit on the assigned seat following the TA/instructor’s instruction\n",
      "  - Place your student ID card on your desk until we finished checking your ID card.\n",
      "- We will collect all exam sharp at 11am. If you arrive late and leave yourself insufficient time of answering questions, you are responsible for the consequences.\n",
      "- We will have a timer and important announcement on the screen. Reading a smartwatch (e.g., AppleWatch, PixelWatch, Fitbit) is considered as cheating.\n",
      "- If you need to use the restroom, please leave all your personal belongs on your seat, including mobile phones.\n",
      "- If you have any question, you have to raise your hand and wait for the TA/instructor to come by. You cannot ask any other during the exam.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('final_exam_logistics', 'The final exam will be on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8f2fbfe1-8453-4632-937b-a9964cb35d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"When is the final exam?\",\n",
    "    \"final_exam_logistics\",\n",
    "    'The final exam will be on'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bbd1638e-184b-4c7c-9155-313ebdc9f264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Assignmnet 4\n",
      "Assignment 4 is scheduled to be released on May 9th and is due on May 23rd.\n",
      "\n",
      "# Assignment 5\n",
      "Assignment 5 is scheduled to be released on May 28th and is due on June 6th.\n",
      "\n",
      "# Reading Quiz 5\n",
      "Reading quiz 5 is scheduled to be released on May 3rd and is due on May 14th.\n",
      "\n",
      "# Reading Quiz 6\n",
      "Reading quiz 6 is scheduled to be released on May 14th and is due on May 21st.\n",
      "\n",
      "# Reading Quiz 7\n",
      "Reading quiz 7 is scheduled to be released on May 21st and is due on May 28th.\n",
      "\n",
      "# Reading Quiz 8\n",
      "Reading quiz 8 is scheduled to be released on May 28th and is due on June 4th.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('release_schedule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0de5c977-c416-4db9-b3fb-f377f7eb935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"When can we expect Assignment 5 to be released?\",\n",
    "    \"release_schedule\",\n",
    "    ''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "42b37f82-6701-4c86-9dc1-659d7f7b239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"When is the deadline of quiz 7?\",\n",
    "    \"release_schedule\",\n",
    "    ''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6842824-7869-4479-8905-58b8c3e1227c",
   "metadata": {},
   "source": [
    "## Troubleshooting FAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4f27fbab-6dfd-4f76-99c5-f98699f67138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Git Pushing Commits\n",
      "\n",
      "## Authentication Failed\n",
      "Question: When I push to GitHub, it gives the error: fatal: Authentication Failed.\n",
      "Answer: Test if your SSH key is setup correctly by executing this command: `ssh -T git@github.com` . If it doesn't respond with \"Hi <YOUR USERNAME>! You've successfully authenticated, but GitHub does not provide shell access.\", refer to assignment 1 \"Set up the cloud credentials\" section. If you have further issues, please contact the TA or the professor.\n",
      "\n",
      "## Not a git repository\n",
      "Question: Tried executing the command in the terminal but an error \"fatal: not a git repository (or any parent up to mount point /) Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\" is occuring constantly.\n",
      "Answer: It seems you execute the code under the root. You should first change working directory to the repo we are using, whose name is like 2023fa-cs203-memory-<your_github_name>\n",
      "\n",
      "\n",
      "# Gradescope Autograder\n",
      "\n",
      "## Question\n",
      "- My output for the programming assignment get 100 with the auto grader in Jupyter Notebook.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('troubleshooting', 'Git Pushing Commits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2360278b-7fcb-416d-9793-265eae47bdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"It says authentication failed when I do git push. What should I do?\",\n",
    "    \"troubleshooting\",\n",
    "    'Git Pushing Commits'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7babf1d0-7128-424b-aa62-ee12d235036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But the auto grader on GradeScope gives a different output\n",
      "- I was getting the correct result and a score of '100' when I ran my code on Jupyter Notebook. But when the autograder is returning the score it is showing 0/100.\n",
      "- I submitted my code on Gradescope before the deadline and the submission fails.\n",
      "- I am facing the following issue when submitting code to the autograder. I received this error: The autograder failed to execute correctly. Contact your course staff for help in debugging this issue. Make sure to include this page so that they can help you most effectively.\n",
      "- When I run the code on datahub, it doesn't show correctness problem. However, running the same code on gradescope is showing below result saying \"Failed tests\". But the solution and reference codes are same. Could you provide details of what Failed tests really mean?\n",
      "\n",
      "## Answer\n",
      "- Gradescope autograder uses a different set of test cases and will give a 0 score if your solution is incorrect.\n",
      "- Every submission that does not meet the performance bar will be considered as “failed” by gradescope.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('troubleshooting', 'Gradescope autograder uses a different set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "10521076-5836-4dfd-a249-89b563a4f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"Gradescope's autograder gives me a different score from what I got in my Jupyter notebook. Why?\",\n",
    "    \"troubleshooting\",\n",
    "    'Gradescope autograder uses a different set'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6b8b6829-18b4-4a54-9da0-93ffaee3298c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If your score is not zero on this screen, it means your outcome is correct.\n",
      "- Gradescope autograder is hosted on a different machine, so the speedup score will not be identical to what you have on Jupyter Hub but will correlate. \n",
      "- Depending on how many active submissions on the server, the speedup score may fluctuate, so you may have worse scores or fail submissions if you submit in the last minute.\n",
      "\n",
      "\n",
      "# Jupyter Hub Down\n",
      "\n",
      "## Question\n",
      "- I've been trying to access DataHub for a while now, but it seems like I can't connect. Is anyone else experiencing the same issue, or is it just on my end? I've tried multiple browsers and checked my internet connection, but the issue persists.\n",
      "- Has anyone else encountered the “500 : Internal Server Error” after signing in to JupyterHub?\n",
      "- Hello Everyone! I was halfway through Assignment #1 and now my server is not working.\n",
      "## Answer\n",
      "Please contact the professor for this issue.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('troubleshooting', 'Jupyter Hub Down')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "86968bba-0ef0-45f5-9d56-0a5fdfa99fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"Jupyter Server says 500: Internal Error. Am I the only one?\",\n",
    "    \"troubleshooting\",\n",
    "    'Jupyter Hub Down'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1e4a9a1a-c152-4e30-bcde-4f8c3e058cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Lecture Recording\n",
      "Question: Where can I find the links for the lecture videos from last year? While checking the course website, there doesn't seem to be any links to videos.\n",
      "Answer: The lecture recording playlist is at YouTube channel Professor Usagi: https://www.youtube.com/playlist?list=PL-8VoXsPtVrt39G3LTYHcq-eaZXltxpwr\n",
      "\n",
      "\n",
      "# Unable to Edit A Notebook Cell\n",
      "\n",
      "## Question\n",
      "- I am unable to edit the answer for question 11 in assignment 1.\n",
      "- While executing the cell, I encountered an error, and it's not allowing me to edit the cell.\n",
      "- While answering Question 12, I noticed that the answer cell is restricted from adding my answer to the question.\n",
      "\n",
      "## Answer\n",
      "To directly solve this issue:\n",
      "- While working on assignment 4, I noticed that the markdown option for question 7 is disabled.\n",
      "1. Turn on the edit metadata button: View > Cell Toolbar > Edit metadata\n",
      "2. Click “Edit Metadata” and change editable to true\n",
      "Alternatively, manually add an answer cell in your turn-in notebook.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('troubleshooting', 'Lecture Recording')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d7dffc79-a94d-4434-8441-313d118bb9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"Where can I see the lecture recording?\",\n",
    "    \"troubleshooting\",\n",
    "    'Lecture Recording'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4360188a-d9e0-4bc8-b980-97a3b0e6c401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Lecture Recording\n",
      "Question: Where can I find the links for the lecture videos from last year? While checking the course website, there doesn't seem to be any links to videos.\n",
      "Answer: The lecture recording playlist is at YouTube channel Professor Usagi: https://www.youtube.com/playlist?list=PL-8VoXsPtVrt39G3LTYHcq-eaZXltxpwr\n",
      "\n",
      "\n",
      "# Unable to Edit A Notebook Cell\n",
      "\n",
      "## Question\n",
      "- I am unable to edit the answer for question 11 in assignment 1.\n",
      "- While executing the cell, I encountered an error, and it's not allowing me to edit the cell.\n",
      "- While answering Question 12, I noticed that the answer cell is restricted from adding my answer to the question.\n",
      "\n",
      "## Answer\n",
      "To directly solve this issue:\n",
      "- While working on assignment 4, I noticed that the markdown option for question 7 is disabled.\n",
      "1. Turn on the edit metadata button: View > Cell Toolbar > Edit metadata\n",
      "2. Click “Edit Metadata” and change editable to true\n",
      "Alternatively, manually add an answer cell in your turn-in notebook.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_documents('troubleshooting', 'Edit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "161b1c36-fd3c-4f00-947e-ed1cae9127bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"How do I edit the question 5 cell in assignment 3?\",\n",
    "    \"troubleshooting\",\n",
    "    'Edit'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8004b4fd-c9c9-40f4-b3ae-5fcb7d08d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = [{'query': q, 'doc_src': s, 'id_phrase': p} for q, s, p in zip(queries, doc_srcs, id_phrases)]\n",
    "with open('data/benchmark.json', 'w') as f:\n",
    "    json.dump(benchmark, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3ea254-86b9-49db-89ee-090a6ffb671b",
   "metadata": {},
   "source": [
    "## Out-of-Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cfce3339-56c2-4aab-ba1d-8f25ac5c43dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ood = [\n",
    "    \"The four prime elementals of the Tensor Processing Unit you invoked - Matrix Weight, Buffer Memory, Accumulatorum Activatus, and Unified Buffer - do they represent the quintessential realms of data, operation, and state?\",\n",
    "    \"It seems a 'process switch' allows transitioning between active demons. What are the procedures for temporarily banishing one while giving breath to another?\",\n",
    "    \"At what size would I expect to see significant increases in L1 and L2 miss rates?\",\n",
    "    \"How many assignments do we have throughout the whole course?\",\n",
    "    \"My PDF of my notebook keeps being cut off. What should I do?\"\n",
    "]\n",
    "\n",
    "with open('data/benchmark_odd.json', 'w') as f:\n",
    "    json.dump(ood, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0f46b30c-31ba-4649-9753-1cc847161ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"The four prime elementals of the Tensor Processing Unit you invoked - Matrix Weight, Buffer Memory, Accumulatorum Activatus, and Unified Buffer - do they represent the quintessential realms of data, operation, and state?\",\n",
    "    \"Readings out-of-distribution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c4fba25a-8f14-4b08-a1c0-5b49ca089480",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"It seems a 'process switch' allows transitioning between active demons. What are the procedures for temporarily banishing one while giving breath to another?\",\n",
    "    \"Textbook out-of-distribution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e14acf9-7937-4ab6-8c16-7e7b7f206da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"At what size would I expect to see significant increases in L1 and L2 miss rates?\",\n",
    "    \"Assignment out-of-distribution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0960c6bd-bd0d-444c-9a05-bb7503de0bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"How many assignments do we have throughout the whole course?\",\n",
    "    \"Logistics out-of-distribution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9ab453da-bc40-4417-8b71-b803065bb49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bmq(\n",
    "    \"My PDF of my notebook keeps being cut off. What should I do?\",\n",
    "    \"Troubleshooting out-of-distribution\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
