{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aecaa24-2a21-48dc-955e-147cdc0958fa",
   "metadata": {},
   "source": [
    "## Convert PDF to Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c14ce9-732c-4a62-963d-f6807207b897",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q marker-pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84cd7d9d-5940-4b89-9678-77cb1eca09fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-10 17:25:08,913\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "Loading detection model vikp/surya_det2 on device cuda with dtype torch.float16\n",
      "Loading detection model vikp/surya_layout2 on device cuda with dtype torch.float16\n",
      "Loading reading order model vikp/surya_order on device cuda with dtype torch.float16\n",
      "Loaded texify model to cuda with torch.float16 dtype\n",
      "Converting 22 pdfs in chunk 1/1 with 5 processes, and storing in /nfshome/tchen307/aqua_rewrite/data/general/readings\n",
      "100%|███████████████████████████████████████████| 22/22 [02:49<00:00,  7.72s/it]\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!DEFAULT_LANG=\"en\" marker data/pdfs/readings data/general/readings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0b7d7e4-5245-43be-b154-eaf8d0f8e97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-10 17:29:32,073\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "Loading detection model vikp/surya_det2 on device cuda with dtype torch.float16\n",
      "Loading detection model vikp/surya_layout2 on device cuda with dtype torch.float16\n",
      "Loading reading order model vikp/surya_order on device cuda with dtype torch.float16\n",
      "Loaded texify model to cuda with torch.float16 dtype\n",
      "Converting 13 pdfs in chunk 1/1 with 5 processes, and storing in /nfshome/tchen307/aqua_rewrite/data/general/textbook\n",
      "100%|███████████████████████████████████████████| 13/13 [01:59<00:00,  9.16s/it]\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!DEFAULT_LANG=\"en\" marker data/pdfs/textbook data/general/textbook/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153cf357-cd71-4e4e-91fa-2793bdb07864",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chunk Data\n",
    "\n",
    "- [LangChain Document Class](https://api.python.langchain.com/en/v0.0.339/schema/langchain.schema.document.Document.html)\n",
    "- [LangChain MarkdownTextSplitter Class](https://api.python.langchain.com/en/latest/markdown/langchain_text_splitters.markdown.MarkdownTextSplitter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5878010e-3630-4146-bb6a-e2baceaec2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_split = MarkdownTextSplitter(chunk_size=256, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afbe45c9-02c9-4e04-b67b-4c073f66ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/final_exam_logistics.md') as f:\n",
    "    md_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca9f2a56-2e86-49eb-9366-a500631dea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = md_split.create_documents([md_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93188d1b-2849-4785-99ba-3cf6f6fcf2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Final Exam Logistics\\n\\n## Time and Location\\n- The location of the final exam is to be announced\\n- The final exam will be on June 14th 8am - 11am'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac72e457-bd5e-4ca7-91d5-686b3845f99f",
   "metadata": {},
   "source": [
    "### Llama Index Splitter\n",
    "\n",
    "LangChain's splitter really sucks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76ba7b24-72b9-4b4c-96c9-00536c9b1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain_core.documents import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97d0cafa-9934-426a-a42e-ab20dd22c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = SentenceSplitter(chunk_size=256, chunk_overlap=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3faeb93c-ff62-4180-821b-b49184fd5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = Path('data/general/readings/A_case_for_partially_TAgged_Geometric_history_length branch_prediction/A_case_for_partially_TAgged_Geometric_history_length branch_prediction.md')\n",
    "with open(doc_path) as f:\n",
    "    doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79312fdb-6b01-42ba-b3a4-9edc26e1fb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='# A Case For (Partially) Tagged Geometric History Length Branch Prediction ⇤\\n\\nAndr´e Seznec seznec@irisa.fr Pierre Michaud pmichaud@irisa.fr IRISA/INRIA/HIPEAC Campus de Beaulieu, 35042 Rennes Cedex, France\\n\\n## Abstract\\n\\nIt is now widely admitted that in order to provide state-of-the-art accuracy, a conditional branch predictor must combine several predictions. Recent research has shown that an adder tree is a very e↵ective approach for the prediction combination function.\\n\\nIn this paper, we present a more cost e↵ective solution for this prediction combination function for predictors relying on several predictor components indexed with di↵erent history lengths. Using GEometric history length as the O-GEHL predictor, the TAGE\\npredictor uses (partially) tagged components as the PPM-like predictor. TAGE relies on\\n(partial) hit-miss detection as the prediction computation function. TAGE provides stateof-the-art prediction accuracy on conditional branches.', metadata={'source': 'A_case_for_partially_TAgged_Geometric_history_length branch_prediction.md'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_chunks = parser.split_text(doc)\n",
    "chunks = [Document(page_content=chunk, metadata={'source': doc_path.name}) for chunk in txt_chunks]\n",
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd12a609-e809-4f87-95bb-04d060a7771d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Embed Data\n",
    "\n",
    "- [BGE Instructions](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "edcfd5c8-3720-4993-b385-5d1fc1e5b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-base-en')\n",
    "model = AutoModel.from_pretrained('BAAI/bge-base-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "994431c6-5356-4081-b677-38e15dfbb26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def bge_embed(text):\n",
    "    input_toks = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    output = model(**input_toks)  # Last hidden state: [1, seq_len, 768]\n",
    "    embd = F.normalize(output.last_hidden_state[:, 0, :], p=2, dim=1)\n",
    "    return embd.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a5af72a-2288-4832-ac81-9da947d0c7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bge_embed(docs[0].page_content).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e7bf3-17e4-4ce1-89a1-1e87315348da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create Index\n",
    "\n",
    "- Couldn't install FAISS\n",
    "- [Spotify Annoy](https://github.com/spotify/annoy)\n",
    "\n",
    "[Tuning `n_trees` and `search_k`](https://github.com/spotify/annoy?tab=readme-ov-file#tradeoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6598cbe7-d383-4b7b-8d80-93925d9e037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "85da28b1-e33e-4fd0-bfb2-9ef75e4bf6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_idx = AnnoyIndex(768, 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bc4697cb-3d49-4c97-901f-aec89a1df045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, doc in enumerate(docs):\n",
    "    embd = bge_embed(doc.page_content)\n",
    "    bge_idx.add_item(i, embd)\n",
    "bge_idx.build(3)\n",
    "bge_idx.save('bge_idx.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "828a5026-1c2f-4d91-a338-9c6c43314c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_idx = AnnoyIndex(768, 'euclidean')\n",
    "bge_idx.load('bge_idx.ann')\n",
    "query = 'Can I bring my own scratch paper?'\n",
    "q_embd = bge_embed(query)\n",
    "nn_vs = bge_idx.get_nns_by_vector(q_embd, 3, include_distances=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "03aff383-6582-4a78-89bb-4f74f33c1841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc=Anything beyond your own pens and calculators are prohibited during the exam. Therefore, you cannot use any of the following during the exam:\n",
      "- You cannot use your own scratch paper\n",
      "- You cannot have a cheatsheet\n",
      "dist=0.5346\n",
      "===========\n",
      "doc=## Items Allowed\n",
      "You may use your own pens and calculators (not those ones on mobile phones).\n",
      "dist=0.6098\n",
      "===========\n",
      "doc=- You cannot borrow/exchange anything during the exam\n",
      "dist=0.6457\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "for i, dist in zip(*nn_vs):\n",
    "    print(f'doc={docs[i].page_content}\\n{dist=:.4f}\\n===========')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5459cb69-9768-4673-a9dd-bd40748024f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Build BGE Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60b1ff1f-2bb0-4983-8abf-fc2dd123cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from annoy import AnnoyIndex\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a6f517b-f590-4a62-9881-f42e035724a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = '(true | false)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11fd795b-5647-4750-9d81-ebcb74d99cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_docs(md_text_dir, chunk_size, chunk_overlap):\n",
    "    md_texts = []\n",
    "    md_split = MarkdownTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    md_text_paths = list(Path(md_text_dir).rglob('*.md'))\n",
    "    for md_text_path in md_text_paths:\n",
    "        with open(md_text_path) as f:\n",
    "            md_text = f.read()\n",
    "        md_texts.append(md_text)\n",
    "    \n",
    "    doc_metadatas = [{'source': md_text_path.name} for md_text_path in md_text_paths]\n",
    "    docs = md_split.create_documents(md_texts, metadatas=doc_metadatas)\n",
    "\n",
    "    return docs\n",
    "\n",
    "docs_ = create_docs('data/quiz', 512, 128)\n",
    "len(docs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea0dd884-963c-4c49-b0cc-44a5be335350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bge_idx():\n",
    "    model_name = 'BAAI/bge-base-en'\n",
    "    hid_dim = 768\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to('cuda')\n",
    "    bge_idx = AnnoyIndex(hid_dim, 'euclidean')\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def bge_embed(text):\n",
    "        input_toks = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "        output = model(**{k: v.to(model.device) for k, v in input_toks.items()})  # Last hidden state: [1, seq_len, 768]\n",
    "        embd = F.normalize(output.last_hidden_state[:, 0, :], p=2, dim=1)\n",
    "        return embd.to('cpu').reshape(-1)\n",
    "\n",
    "    return bge_embed, bge_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b2f7052-50c4-409e-b579-d60c9d543609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bge_idx(docs, n_trees):\n",
    "    bge_embed, bge_idx = init_bge_idx()\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        embd = bge_embed(doc.page_content)\n",
    "        bge_idx.add_item(i, embd)\n",
    "    \n",
    "    bge_idx.build(n_trees)\n",
    "    bge_idx.save('bge_idx.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81e6a77f-39bf-4d4f-a89b-443d4b8f2717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_query_bge_idx(docs, top_k):\n",
    "    bge_embed, bge_idx = init_bge_idx()\n",
    "\n",
    "    file_name = 'bge_idx.ann'\n",
    "    assert Path(file_name).exists(), f'Index {file_name} not found.'\n",
    "    bge_idx.load(file_name)\n",
    "\n",
    "    def query_bge_idx(query):\n",
    "        '''Output: List[tuple(float, int)]: (distance, document idx)'''\n",
    "        q_embd = bge_embed(query)\n",
    "        idxs, dists = bge_idx.get_nns_by_vector(q_embd, top_k, include_distances=True)\n",
    "\n",
    "        top_docs = []\n",
    "        for dist, idx in sorted(zip(dists, idxs), reverse=True):\n",
    "            top_doc = docs[idx].copy(update={'dist': dist})\n",
    "            top_docs.append(top_doc)\n",
    "\n",
    "        return top_docs\n",
    "\n",
    "    return query_bge_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe5d0398-a078-4031-9206-8b1facee4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bge_idx(docs_, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af7ee007-82e7-44f5-a12b-0f65f6404cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: or if the anticipated branch address were found to be incorrect, a small *gshare* table would be consulted for a quick prediction. The study shows that a similar predictor, using two *gshare* tables, is able to use the larger table 47% of the time.\n",
      "source: dynamic_branch_prediction_with_perceptrons.md\n",
      "dist=0.5237\n",
      "==============================\n",
      "doc: # A Study Of Branch Prediction Strategies\n",
      "\n",
      "JAMES E. SMITH \n",
      "Control Data Corporation Arden Hills, Minnesota \n",
      "\n",
      "## Abstract\n",
      "source: a_study_of_branch_prediction_strategies.md\n",
      "dist=0.5221\n",
      "==============================\n",
      "doc: However, in Section 4, we present the simulation results of the TAGE predictor strictly respecting the 1st Championship Branch Prediction Rules.\n",
      "\n",
      "## 2.3 Information Used For Indexing The Branch Predictor\n",
      "\n",
      "For computing the indexes for global history predictors, most studies consider either hashing the conditional branch history with the branch address or hashing the path history with the branch address [22]. Both these solutions lead to consider distinct paths as equal.\n",
      "source: a_case_for_partially_tagged_geometric_history_length_branch_prediction.md\n",
      "dist=0.5216\n",
      "==============================\n",
      "doc: For a 8-component TAGE predictor, we use respectively 9-bit tags for T1 and T2, 10-bit tags for T3 and T4, 11-bit tags for T5 and T6, 12-bit tags for T7. The tagged tables feature 512 entries, and represent a total of 53.5 Kbits.\n",
      "source: a_case_for_partially_tagged_geometric_history_length_branch_prediction.md\n",
      "dist=0.5150\n",
      "==============================\n",
      "doc: We present the TAGE conditional branch predictor. TAGE stands for TAgged GEometric history length. TAGE is derived from Michaud's tagged PPM-like predictor [20]. It relies on a default tagless predictor backed with a plurality of (partially) tagged predictor components indexed using di↵erent history lengths for index computation. These history lengths form a geometric series. The prediction is provided either by a tag match on a tagged predictor component or by the default predictor. In case of multiple\n",
      "source: a_case_for_partially_tagged_geometric_history_length_branch_prediction.md\n",
      "dist=0.5079\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "query_bge_idx = init_query_bge_idx(docs_, 5)\n",
    "query = 'how does TAGE predictor match branch with a table.'\n",
    "top_docs_ = query_bge_idx(query)\n",
    "for top_doc in top_docs_:\n",
    "    print(f'doc: {top_doc.page_content}\\nsource: {top_doc.metadata[\"source\"]}\\ndist={top_doc.dist:.4f}')\n",
    "    print('='*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd33c75-f699-412b-b35b-eee817e26be8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Build GTE Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c0d7a91-e596-470b-b48f-b83b5817d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT is an encoder Transformer, the attention mask is always one during inference\n",
    "model_name = 'thenlper/gte-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to('cuda')\n",
    "for doc_ in docs_:\n",
    "    input_toks = tokenizer(doc_.page_content, padding=True, truncation=True, return_tensors='pt')\n",
    "    if not torch.allclose(input_toks['attention_mask'], torch.ones_like(input_toks['attention_mask'])):\n",
    "        print('Gotcha!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ba2ee5-60d7-4a4d-836e-994079311259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gte_idx():\n",
    "    model_name = 'thenlper/gte-base'\n",
    "    hid_dim = 768\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to('cuda')\n",
    "    gte_idx = AnnoyIndex(hid_dim, 'euclidean')\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def gte_embed(text):\n",
    "        input_toks = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "        output = model(**{k: v.to(model.device) for k, v in input_toks.items()})  # Last hidden state: [1, seq_len, 768]\n",
    "        embd = F.normalize(output.last_hidden_state.mean(dim=1), p=2, dim=1)\n",
    "        return embd.to('cpu').reshape(-1)\n",
    "\n",
    "    return gte_embed, gte_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3ca1be0-9cad-408a-838b-31f173ac8c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gte_idx(docs, n_trees):\n",
    "    gte_embed, gte_idx = init_gte_idx()\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        embd = gte_embed(doc.page_content)\n",
    "        gte_idx.add_item(i, embd)\n",
    "    \n",
    "    gte_idx.build(n_trees)\n",
    "    gte_idx.save('gte_idx.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43330340-3243-4883-9b23-94cd9a1a81f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_query_gte_idx(docs, top_k):\n",
    "    gte_embed, gte_idx = init_gte_idx()\n",
    "\n",
    "    file_name = 'gte_idx.ann'\n",
    "    assert Path(file_name).exists(), f'Index {file_name} not found.'\n",
    "    gte_idx.load(file_name)\n",
    "\n",
    "    def query_gte_idx(query):\n",
    "        '''Output: List[tuple(float, int)]: (distance, document idx)'''\n",
    "        q_embd = gte_embed(query)\n",
    "        idxs, dists = gte_idx.get_nns_by_vector(q_embd, top_k, include_distances=True)\n",
    "\n",
    "        top_docs = []\n",
    "        for dist, idx in sorted(zip(dists, idxs), reverse=True):\n",
    "            top_doc = docs[idx].copy(update={'dist': dist})\n",
    "            top_docs.append(top_doc)\n",
    "\n",
    "        return top_docs\n",
    "\n",
    "    return query_gte_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25541e6b-451e-4cf6-a63c-b0b15e70fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gte_idx(docs_, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d54ef042-e888-4775-a1c9-40cec02218e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: The last stage in the prediction computation on the TAGE predictor consists in the tag match followed by the prediction selection. The tag match computations are performed in parallel on the tags flowing out from the tagged components.\n",
      "source: a_case_for_partially_tagged_geometric_history_length_branch_prediction.md\n",
      "dist=0.4527\n",
      "==============================\n",
      "doc: However, in Section 4, we present the simulation results of the TAGE predictor strictly respecting the 1st Championship Branch Prediction Rules.\n",
      "\n",
      "## 2.3 Information Used For Indexing The Branch Predictor\n",
      "\n",
      "For computing the indexes for global history predictors, most studies consider either hashing the conditional branch history with the branch address or hashing the path history with the branch address [22]. Both these solutions lead to consider distinct paths as equal.\n",
      "source: a_case_for_partially_tagged_geometric_history_length_branch_prediction.md\n",
      "dist=0.4506\n",
      "==============================\n",
      "doc: We present the TAGE conditional branch predictor. TAGE stands for TAgged GEometric history length. TAGE is derived from Michaud's tagged PPM-like predictor [20]. It relies on a default tagless predictor backed with a plurality of (partially) tagged predictor components indexed using di↵erent history lengths for index computation. These history lengths form a geometric series. The prediction is provided either by a tag match on a tagged predictor component or by the default predictor. In case of multiple\n",
      "source: a_case_for_partially_tagged_geometric_history_length_branch_prediction.md\n",
      "dist=0.4378\n",
      "==============================\n",
      "doc: predictor uses (partially) tagged components as the PPM-like predictor. TAGE relies on\n",
      "(partial) hit-miss detection as the prediction computation function. TAGE provides stateof-the-art prediction accuracy on conditional branches. In particular, at equivalent storage budgets, the TAGE predictor significantly outperforms all the predictors that were presented at the Championship Branch Prediction in december 2004.\n",
      "source: a_case_for_partially_tagged_geometric_history_length_branch_prediction.md\n",
      "dist=0.4275\n",
      "==============================\n",
      "doc: ## 3.2 The Tage Predictor\n",
      "\n",
      "The TAGE predictor is directly derived from Michaud's PPM-like tag-based branch predictor [20]. Figure 1 illustrates a TAGE predictor. The TAGE predictor features a base predictor T0 in charge of providing a basic prediction and a set of (partially) tagged predictor components Ti. These tagged predictor components Ti, 1  i  M are indexed using di↵erent history lengths that form a geometric series, i.e L(i)=(int)(↵i1⇤L(1)+0.5).\n",
      "source: a_case_for_partially_tagged_geometric_history_length_branch_prediction.md\n",
      "dist=0.4257\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "query_gte_idx = init_query_gte_idx(docs_, 5)\n",
    "top_docs_ = query_gte_idx('how does TAGE predictor match branch with a table.')\n",
    "for top_doc in top_docs_:\n",
    "    print(f'doc: {top_doc.page_content}\\nsource: {top_doc.metadata[\"source\"]}\\ndist={top_doc.dist:.4f}')\n",
    "    print('='*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536e0942-44fa-4fee-a6d5-b6d03d75f0a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LLM\n",
    "\n",
    "[StableLM Zephyr 3B](https://huggingface.co/stabilityai/stablelm-zephyr-3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc50cfac-259e-4da6-9cd7-66cd987d4b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cc43404-a402-4b7e-86cf-b3662ef22f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'stabilityai/stablelm-zephyr-3b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1092972-e7fe-4ae2-81d3-a5eae4a39542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or if the anticipated branch address were found to be incorrect, a small *gshare* table would be consulted for a quick prediction. The study shows that a similar predictor, using two *gshare* tables, is able to use the larger table 47% of the time.\n",
      "---\n",
      "# A Study Of Branch Prediction Strategies\n",
      "\n",
      "JAMES E. SMITH \n",
      "Control Data Corporation Arden Hills, Minnesota \n",
      "\n",
      "## Abstract\n",
      "---\n",
      "However, in Section 4, we present the simulation results of the TAGE predictor strictly respecting the 1st Championship Branch Prediction Rules.\n",
      "\n",
      "## 2.3 Information Used For Indexing The Branch Predictor\n",
      "\n",
      "For computing the indexes for global history predictors, most studies consider either hashing the conditional branch history with the branch address or hashing the path history with the branch address [22]. Both these solutions lead to consider distinct paths as equal.\n",
      "---\n",
      "For a 8-component TAGE predictor, we use respectively 9-bit tags for T1 and T2, 10-bit tags for T3 and T4, 11-bit tags for T5 and T6, 12-bit tags for T7. The tagged tables feature 512 entries, and represent a total of 53.5 Kbits.\n",
      "---\n",
      "We present the TAGE conditional branch predictor. TAGE stands for TAgged GEometric history length. TAGE is derived from Michaud's tagged PPM-like predictor [20]. It relies on a default tagless predictor backed with a plurality of (partially) tagged predictor components indexed using di↵erent history lengths for index computation. These history lengths form a geometric series. The prediction is provided either by a tag match on a tagged predictor component or by the default predictor. In case of multiple\n"
     ]
    }
   ],
   "source": [
    "context = '\\n---\\n'.join(top_doc.page_content for top_doc in top_docs_)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7916830b-462c-4834-94c1-8fa154aa817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "instr = '''\\\n",
    "Context information is below.\n",
    "===\n",
    "{context}\n",
    "===\n",
    "Given the context information above and not prior knowledge, answer the query.\n",
    "Respond \"Sorry I cannot answer that\" if no relevant information is in the context.\n",
    "Query: {query}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad2c6e3d-141b-4638-9327-654eb997bbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt = [{'role': 'user', 'content': instr.format(context=context, query=query)}]\n",
    "in_toks = tokenizer.apply_chat_template(prompt, add_generation_prompt=True, return_tensors='pt')\n",
    "out_toks = model.generate(in_toks.to(model.device), max_new_tokens=256, temperature=0.0)\n",
    "answer = tokenizer.decode(out_toks[0, ...])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
